{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_type = '17'\n",
    "normalized = 'done'\n",
    "erased_word = '' #JSNCFAO\n",
    "num_word = 8000\n",
    "import pickle\n",
    "import os\n",
    "path = os.getcwd()+'/pickles/'\n",
    "with open(path+data_type+'training.pickle', 'rb') as f:\n",
    "    training = pickle.load(f)\n",
    "with open(path+data_type+'test.pickle', 'rb') as f:\n",
    "    test = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:\\\\Users\\\\tmdgp\\\\python\\\\stock2/pickles/17sel_words8000_done.pickle'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-5aa44c33bec3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mdata_type\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'sel_words'\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_word\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'_'\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mnormalized\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0merased_word\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'.pickle'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m     \u001b[0mselected_words\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:\\\\Users\\\\tmdgp\\\\python\\\\stock2/pickles/17sel_words8000_done.pickle'"
     ]
    }
   ],
   "source": [
    "with open(path+data_type+'sel_words'+str(num_word)+'_'+normalized+erased_word+'.pickle', 'rb') as f:\n",
    "    selected_words = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training set의 단어들만 따로 모아 학습에 포함할 단어 선별\n",
    "import nltk\n",
    "token = [t for d in training for t in d[0]] #단어들만 따로 모음\n",
    "text = nltk.Text(token)\n",
    "selected_words = [f[0] for f in text.vocab().most_common(num_word+(1000*len(erased_word)))] #상위 6000 빈도수 단어\n",
    "\n",
    "if erased_word != '':\n",
    "    word_for_remove = set()\n",
    "    for word in selected_words:\n",
    "        if 'J' in erased_word:\n",
    "            if '/Josa' in word:\n",
    "                word_for_remove.add(word)\n",
    "        if 'S' in erased_word:\n",
    "            if '/Suffix' in word:\n",
    "                word_for_remove.add(word)\n",
    "        if 'N' in erased_word:\n",
    "            if '/Number' in word:\n",
    "                word_for_remove.add(word)\n",
    "        if 'C' in erased_word:\n",
    "            if '/Conjunction' in word:\n",
    "                word_for_remove.add(word)\n",
    "        if 'F' in erased_word:\n",
    "            if '/Foreign' in word:\n",
    "                word_for_remove.add(word)\n",
    "        if 'A' in erased_word:\n",
    "            if '/Adverb' in word:\n",
    "                word_for_remove.add(word)\n",
    "        if 'O' in erased_word:\n",
    "            if word[1] == '/':\n",
    "                word_for_remove.add(word)\n",
    "    for word in word_for_remove:\n",
    "        selected_words.remove(word)\n",
    "\n",
    "    # 수작업을 통한 필요한 단어 추가\n",
    "    #selected_words.append('없이/Adverb')\n",
    "        \n",
    "    # 수작업을 통한 불필요한 단어 제거\n",
    "    #selected_words.remove('하다/Verb')\n",
    "    #selected_words.remove('있다/Adjective')\n",
    "    #selected_words.remove('되다/Verb')\n",
    "    #selected_words.remove('돼다/Verb')\n",
    "    #selected_words.remove('이다/Verb')\n",
    "selected_words = selected_words[:num_word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_x_tmp is done\n",
      "test_x_tmp is done\n",
      "train_sum is done\n",
      "test_sum is done\n",
      "train_x is done\n",
      "test_x is done\n"
     ]
    }
   ],
   "source": [
    "# 선정된 각 단어들의 기사별 빈도수를 input data의 x값으로 정리\n",
    "def term_frequency(doc):\n",
    "    return [doc.count(word) for word in selected_words]\n",
    "\n",
    "\n",
    "train_x_tmp = [(term_frequency(a),d) for a,_,d in training]\n",
    "print('train_x_tmp is done')\n",
    "test_x_tmp = [(term_frequency(a),d) for a,_,d in test]\n",
    "print('test_x_tmp is done')\n",
    "\n",
    "if normalized == 'done':\n",
    "    # x 리스트의 총 합이 1이 되도록 normalize 하는 과정\n",
    "    train_sum = [sum(d[0]) for d in train_x_tmp]\n",
    "    print('train_sum is done')\n",
    "    test_sum = [sum(d[0]) for d in test_x_tmp]\n",
    "    print('test_sum is done')\n",
    "    \n",
    "    for i in range(len(train_sum)):\n",
    "        if train_sum[i] == 0:\n",
    "            train_sum[i] = 1\n",
    "    for i in range(len(test_sum)):\n",
    "        if test_sum[i] == 0:\n",
    "            test_sum[i] = 1\n",
    "    \n",
    "    \n",
    "    train_x = []\n",
    "    train2_x = []    \n",
    "    for d in range(len(train_x_tmp)):\n",
    "        li = [float(l)/train_sum[d] for l in train_x_tmp[d][0]]\n",
    "        train_x.append(li)\n",
    "        train2_x.append((li,train_x_tmp[d][1]))\n",
    "    print('train_x is done')\n",
    "    test_x = []\n",
    "    test2_x = []\n",
    "    for d in range(len(test_x_tmp)):\n",
    "        li = [float(l)/test_sum[d] for l in test_x_tmp[d][0]]\n",
    "        test_x.append(li)\n",
    "        test2_x.append((li,test_x_tmp[d][1]))\n",
    "    print('test_x is done')\n",
    "elif normalized == 'not':\n",
    "    train_x = [a for a,_ in train_x_tmp]\n",
    "    train2_x = train_x_tmp\n",
    "    test_x = [a for a,_ in test_x_tmp]\n",
    "    test2_x = test_x_tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input data의 y값 정리\n",
    "def make_y_list(data):\n",
    "    if data == 0:\n",
    "        return [0,1,0]\n",
    "    elif data == 1:\n",
    "        return [0,0,1]\n",
    "    elif data == -1:\n",
    "        return [1,0,0]\n",
    "train_y = [make_y_list(c) for _,c,_ in training]\n",
    "test_y = [make_y_list(c) for _,c,_ in test]\n",
    "train2_y = {}\n",
    "test2_y = {}\n",
    "for _,c,d in training:\n",
    "    train2_y[d]= int(c)\n",
    "for _,c,d in test:\n",
    "    test2_y[d]= int(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 생성된 training, test의 x,y값을 pickle의 형태로 저장\n",
    "import pickle\n",
    "import os\n",
    "path = os.getcwd()+'/pickles/'\n",
    "if erased_word != '':\n",
    "    erased_word = '_'+erased_word\n",
    "with open(path+data_type+'train_x_'+str(num_word)+'_'+normalized+erased_word+'.pickle', 'wb') as f:\n",
    "    pickle.dump(train_x, f)\n",
    "with open(path+data_type+'train_y_'+str(num_word)+'_'+normalized+erased_word+'.pickle', 'wb') as f:\n",
    "    pickle.dump(train_y, f)\n",
    "with open(path+data_type+'train2_x_'+str(num_word)+'_'+normalized+erased_word+'.pickle', 'wb') as f:\n",
    "    pickle.dump(train2_x, f)\n",
    "with open(path+data_type+'train2_y_'+str(num_word)+'_'+normalized+erased_word+'.pickle', 'wb') as f:\n",
    "    pickle.dump(train2_y, f)\n",
    "with open(path+data_type+'test_x_'+str(num_word)+'_'+normalized+erased_word+'.pickle', 'wb') as f:\n",
    "    pickle.dump(test_x, f)\n",
    "with open(path+data_type+'test_y_'+str(num_word)+'_'+normalized+erased_word+'.pickle', 'wb') as f:\n",
    "    pickle.dump(test_y, f)\n",
    "with open(path+data_type+'test2_x_'+str(num_word)+'_'+normalized+erased_word+'.pickle', 'wb') as f:\n",
    "    pickle.dump(test2_x, f)\n",
    "with open(path+data_type+'test2_y_'+str(num_word)+'_'+normalized+erased_word+'.pickle', 'wb') as f:\n",
    "    pickle.dump(test2_y, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path+data_type+'sel_words'+str(num_word)+'_'+normalized+erased_word+'.pickle', 'wb') as f:\n",
    "    pickle.dump(selected_words, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
