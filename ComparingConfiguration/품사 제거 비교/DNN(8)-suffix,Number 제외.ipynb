{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 사용할 데이터의 형태\n",
    "data_type = '8'      # '1' : summarize X,개별 기사   '2' : summarize X,날짜별 기사\n",
    "                    # '3' : summarize O,개별 기사   '4' : summarize O,날짜별 기사\n",
    "normalized = 'done' # 'done' : 데이터 행별 normalize한 데이터     'not' : 데이터 행별 normalize 안한 데이터\n",
    "erased_word = 'SN'    # 제거된 단어 형태\n",
    "num_word = '6000'   # 사용할 단어 수\n",
    "\n",
    "# 돌려볼 모델의 구조\n",
    "first_layer = 3000   # 모델의 첫 번째 레이어 수\n",
    "second_layer = 1000  # 모델의 두 번째 레이어 수\n",
    "ep = 60             # 모델의 에포크 수\n",
    "ba = 1000            # 모델의 배치 사이즈"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "EOFError",
     "evalue": "Ran out of input",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mEOFError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-ddbce98f273c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0merased_word\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'_'\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0merased_word\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mdata_type\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'train_x_'\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_word\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'_'\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mnormalized\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0merased_word\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'.pickle'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[0mtrain_x\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mdata_type\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'train_y_'\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_word\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'_'\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mnormalized\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0merased_word\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'.pickle'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0mtrain_y\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mEOFError\u001b[0m: Ran out of input"
     ]
    }
   ],
   "source": [
    "# 생성된 training, test의 x,y값을 pickle의 형태로 저장\n",
    "import pickle\n",
    "import os\n",
    "path = os.getcwd()+'/pickles/'\n",
    "if erased_word != '':\n",
    "    erased_word = '_'+erased_word\n",
    "with open(path+data_type+'train_x_'+str(num_word)+'_'+normalized+erased_word+'.pickle', 'rb') as f:\n",
    "    train_x = pickle.load(f)\n",
    "with open(path+data_type+'train_y_'+str(num_word)+'_'+normalized+erased_word+'.pickle', 'rb') as f:\n",
    "    train_y = pickle.load(f)\n",
    "with open(path+data_type+'train2_x_'+str(num_word)+'_'+normalized+erased_word+'.pickle', 'rb') as f:\n",
    "    train2_x = pickle.load(f)\n",
    "with open(path+data_type+'train2_y_'+str(num_word)+'_'+normalized+erased_word+'.pickle', 'rb') as f:\n",
    "    train2_y = pickle.load(f)\n",
    "with open(path+data_type+'test_x_'+str(num_word)+'_'+normalized+erased_word+'.pickle', 'rb') as f:\n",
    "    test_x = pickle.load(f)\n",
    "with open(path+data_type+'test_y_'+str(num_word)+'_'+normalized+erased_word+'.pickle', 'rb') as f:\n",
    "    test_y = pickle.load(f)\n",
    "with open(path+data_type+'test2_x_'+str(num_word)+'_'+normalized+erased_word+'.pickle', 'rb') as f:\n",
    "    test2_x = pickle.load(f)\n",
    "with open(path+data_type+'test2_y_'+str(num_word)+'_'+normalized+erased_word+'.pickle', 'rb') as f:\n",
    "    test2_y = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# numpy 사용하여 float로 바꾸어주는 과정\n",
    "import numpy as np\n",
    "x_train = np.asarray(train_x).astype('float32')\n",
    "y_train = np.asarray(train_y).astype('float32')\n",
    "x_test = np.asarray(test_x).astype('float32')\n",
    "y_test = np.asarray(test_y).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# DNN 학습 모델\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras import losses\n",
    "from tensorflow.keras import metrics\n",
    "#from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "\n",
    "#callbacks = [EarlyStopping(monitor='val_loss',patience = 20)]\n",
    "model = models.Sequential()\n",
    "n =len(x_train[0])\n",
    "model.add(layers.Dense(first_layer,activation='relu'))#실제 쓸 단어갯수\n",
    "model.add(layers.Dense(second_layer,activation='relu'))\n",
    "model.add(layers.Dense(3,activation='softmax'))\n",
    "model.compile(optimizer=optimizers.RMSprop(lr=0.001),loss = losses.categorical_crossentropy,metrics=[metrics.categorical_accuracy])\n",
    "\n",
    "#hist = model.fit(x_train,y_train,epochs=ep,validation_data=(x_train2,y_train2),callbacks=callbacks,batch_size=ba, shuffle=True)\n",
    "hist = model.fit(x_train,y_train,epochs=ep,validation_data=(x_test,y_test),batch_size=ba)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델의 예측값을 튜플로 반환하는 함수\n",
    "def predict(x):\n",
    "    data = np.expand_dims(np.asarray(x).astype('float32'),axis=0)\n",
    "    result = model.predict(data)\n",
    "    return tuple(result[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모든 날짜의 예측값들을 합치기 위한 튜플의 합 함수\n",
    "def sum_tuple(a,b):\n",
    "    return (a[0]+b[0],a[1]+b[1],a[2]+b[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, acc_ax = plt.subplots()\n",
    "\n",
    "loss_ax = acc_ax.twinx()\n",
    "\n",
    "loss_ax.plot(hist.history['loss'], 'y', label='train loss')\n",
    "loss_ax.plot(hist.history['val_loss'], 'r', label='val loss')\n",
    "\n",
    "acc_ax.plot(hist.history['categorical_accuracy'], 'b', label='train acc')\n",
    "acc_ax.plot(hist.history['val_categorical_accuracy'], 'g', label='val acc')\n",
    "\n",
    "loss_ax.set_xlabel('epoch')\n",
    "loss_ax.set_ylabel('loss')\n",
    "acc_ax.set_ylabel('accuray')\n",
    "\n",
    "loss_ax.legend(loc='upper left')\n",
    "acc_ax.legend(loc='lower left')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test set에 대한 예측 및 예측 정확도의 계산\n",
    "import re\n",
    "\n",
    "count = 0\n",
    "sum_dic = {}\n",
    "for data in train2_x:\n",
    "    result = predict(data[0])\n",
    "    if data[1] in sum_dic:\n",
    "        sum_dic[data[1]] = sum_tuple(sum_dic[data[1]],result)\n",
    "    else:\n",
    "        sum_dic[data[1]] = result\n",
    "    \n",
    "for date in sum_dic:\n",
    "    real = train2_y[date]\n",
    "    value = sum_dic[date].index(max(sum_dic[date]))\n",
    "    value -= 1\n",
    "    if real == value:\n",
    "        count = count + 1\n",
    "train_score = count / len(sum_dic)\n",
    "\n",
    "count = 0\n",
    "sum_dic = {}\n",
    "for data in test2_x:\n",
    "    result = predict(data[0])\n",
    "    if data[1] in sum_dic:\n",
    "        sum_dic[data[1]] = sum_tuple(sum_dic[data[1]],result)\n",
    "    else:\n",
    "        sum_dic[data[1]] = result\n",
    "    \n",
    "for date in sum_dic:\n",
    "    real = test2_y[date]\n",
    "    value = sum_dic[date].index(max(sum_dic[date]))\n",
    "    value -= 1\n",
    "    if real == value:\n",
    "        count = count + 1\n",
    "test_score = count / len(sum_dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('단어 갯수 : '+num_word)\n",
    "print('모델 레이어 및 노드 수 : ','첫 번째 레이어 - relu,',first_layer,', 두 번째 레이어 - relu,',second_layer)\n",
    "print('epochs : ',ep,', batch_size : ', ba)\n",
    "print('train 데이터 정확도 : ', train_score)\n",
    "print('test 데이터 정확도 : ', test_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
